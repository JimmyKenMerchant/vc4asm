# BCM2835 "GPU_FFT" release 3.0
#
# Copyright (c) 2015, Andrew Holme.
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form must reproduce the above copyright
#       notice, this list of conditions and the following disclaimer in the
#       documentation and/or other materials provided with the distribution.
#     * Neither the name of the copyright holder nor the
#       names of its contributors may be used to endorse or promote products
#       derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY
# DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
# (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
# LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
# ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

##############################################################################
# Bit-rotated write

.set PASS16_STRIDE, ((1<<STAGES)/16*8)
.set PASS32_STRIDE, ((1<<STAGES)/32*8)
.set PASS64_STRIDE, ((1<<STAGES)/64*8)

##############################################################################
# Load twiddle factors

# (MM) Optimized: calculate twiddle offsets for each element once at startup
.macro init_tw
    shl r0, elem_num, 3
    add rx_tw_shared, unif, r0
    add rx_tw_unique, unif, r0
.endm

# (MM) Optimized seperate preparation of TMU from ldtmu for better pipeline processing.
# prepare twiddle load
#   ptr : twiddle base pointer
#   src : twiddle offset (logical)
.macro read_tw, ptr, src
    # (MM) Optimized: do not calculate compile time constant at run time
    # and calculate twiddle offsets for each element once at startup
.if src != 0
    ;mov r0, src << 7
    add t0s, ptr, r0;  v8adds r0, r0, 4
    add t0s, ptr, r0;
.elseif !isRegfileB(ptr)
    mov t0s, ptr; add r0, ptr, 4
    mov t0s, r0;
.else
    mov t0s, ptr; mov r0, ptr
    add t0s, r0, 4;
.endif
.endm

# store prepared twiddles into register
#   ra_tw_re, rb_tw_im : real and imaginary register set start
#   dst : register offset
#   clr_res : clear residual accumulators at dst+1
.macro store_tw, dst, clr_res
.if clr_res
                        ;  mov ra_tw_re+dst+1, 0;  ldtmu0
    mov ra_tw_re+dst, r4;  mov rb_tw_im+dst+1, 0;  ldtmu0
.else
                        ;  ldtmu0
    mov ra_tw_re+dst, r4;  ldtmu0
.endif
    mov rb_tw_im+dst, r4;
.endm

##############################################################################
# VPM pointers

# (MM) Optimized: reduced VPM registers to 1
.macro inst_vpm, in_inst, out_vpm
    # (MM) Optimized: instruction packing
    .assert vpm_setup(1, 1, v32(0,2)) - vpm_setup(1, 1, v32(0,0)) == 2
    ;v8adds r1, in_inst, in_inst
    mov r0, vpm_setup(1, 1, v32(0,0))
    add out_vpm, r0, r1;
.endm

##############################################################################

# (MM) macro proc no longer used

# (MM) Optimized: clone instructions of branch target instead of nops.
.macro brr_opt, dest, target, back
    .assert back >= 0 && back <= 3
    .back back
    brr dest, target + 8 * (3 - back)
    .endb
    .clone target, (3-back)
.endm

##############################################################################

.macro write_vpm_16
    # (MM) Optimized: allow rb register for vpm setup
    ;mov vw_setup, rx_vpm
    mov vpm, r0
    mov vpm, r1;
.endm

.macro write_vpm_32
    # (MM) Optimized: no need for 2 VPM registers anymore, less memory I/O
    # allow rb register for vpm setup
    mov vw_setup, rx_vpm;   mov r2, rx_vpm
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, ra_32_re, r0
    fadd vpm, rb_32_im, r1; mov r3, rb_32_im
    add vw_setup, r2, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    fsub vpm, ra_32_re, r0
    fsub vpm, r3,       r1;
.endm

.macro write_vpm_64
    # (MM) Optimized: no need for 4 VPM registers anymore
    .if isRegfileB(rx_vpm)
    mov vw_setup, rx_vpm; mov r0, rx_vpm
    .else
    add r0, rx_vpm, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, rx_vpm
    .endif
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, ra_64+0, rb_64+0
    fadd vpm, ra_64+1, rb_64+1; mov r1, rb_64+1 
    .if isRegfileB(rx_vpm)
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    .endif
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fadd vpm, ra_64+2, rb_64+2; mov r2, rb_64+2
    fadd vpm, ra_64+3, rb_64+3; mov r3, rb_64+3
    add r0, r0, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)); mov vw_setup, r0
    fsub vpm, ra_64+0, rb_64+0
    fsub vpm, ra_64+1, r1
    mov vw_setup, r0
    fsub vpm, ra_64+2, r2
    fsub vpm, ra_64+3, r3;
.endm

##############################################################################

# (MM) Unified vpm register names
.macro body_ra_save_16, arg_vdw
    # (MM) Optimized: move write_vpm_16 to body_pass_16

    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    bra -, ra_link_1

    # (MM) Optimized: no need for rb_0x40
    mov r0, 0x40; mov vw_setup, arg_vdw
    mov vw_setup, vdw_setup_1(0) + PASS16_STRIDE-16*4
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr
.endm

##############################################################################

# (MM) Unified vpm register names
.macro body_rx_save_slave
    # (MM) Optimized: move write_vpm_16 to body_pass_16
    # => now the same as save_slave_32

    # (MM) Optimized: easier procedure chains
    mov vr_setup, rx_vpm
    brr -, r:sync, ra_sync
    nop
    nop
    mov -, vpm
.endm

##############################################################################

.macro body_ra_save_32
    # (MM) Optimized: move write_vpm_32 to body_pass_32

    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
        mov -, srel(i+1) # Master releases slave
    .endr

    bra -, ra_link_1

    # (MM) Optimized: no need for rb_0x40
    mov r0, 0x40;                     mov vw_setup, ra_vdw_32
    mov vw_setup, vdw_setup_1(0) + PASS32_STRIDE-16*4
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr
.endm

##############################################################################

# (MM) no longer used
#.macro body_rx_save_slave_32

##############################################################################

.macro body_ra_save_64
    mov -, vw_wait

    .rep i, 7
        mov -, srel(i+1) # Master releases slaves
    .endr

    write_vpm_64

    # (MM) Optimized: no need for rb_0x40
    ;mov r0, 0x40 # joins with write_vpm

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slaves
    .endr

    mov vw_setup, vdw_setup_0(64, 16, dma_h32(0,0))
    mov vw_setup, vdw_setup_1(0) + PASS64_STRIDE-16*4
    add ra_save_ptr, ra_save_ptr, r0; mov vw_addr, ra_save_ptr
    .back 3
    bra -, ra_link_1
    .endb
.endm

##############################################################################

.macro body_rx_save_slave_64
    # (MM) Optimized:
    # - Calculate entry point at setup like for body_rx_sync_slave,
    # saves 7 instructions
    # - Scheduled code of write_vpm_64 with branch instructions,
    # saves another 5 instructions.
    # - Removed unnecessary code for semaphore 0 (8 instructions dead code).
    # - Furthermore store the return address as offset for the second branch to srel.
    .rep i, 7
        mov -, sacq(i+1) # Slave waits for master
        brr_opt ra_temp, r:2f, 1
    .endr
:2
    write_vpm_64

    mov vr_setup, r0
    nop
    .back 3
       # (MM) Optimized: Take the return address from above as base of the branch target.
       bra -, ra_temp, :3f - :2 - 4*8
    .endb

    .rep i, 7
        bra -, ra_link_1
        nop
        mov -, vpm
        mov -, srel(i+9) # Slave releases master
    .endr
:3
.endm

##############################################################################

.macro body_ra_sync
    mov -, vw_wait

    .rep i, 7
        mov -, sacq(i+9) # Master waits for slave
    .if i==5
        bra -, ra_link_1
    .endif
        mov -, srel(i+1) # Master releases slave
    .endr
.endm

##############################################################################

.macro body_rx_sync_slave
    .rep i, 7
        bra -, ra_link_1
        mov -, srel(i+9) # Slave releases master
        # (MM) Optimized: wait as late as possible
        nop
        mov -, sacq(i+1) # Slave waits for master
    .endr
.endm

##############################################################################

.macro fft_twiddles_32
    # (MM) Optimized: less memory I/O
    ;                fmul r3, r0, ra_tw_re+TW32 # rr
    mov r2, r0;      fmul r0, r1, rb_tw_im+TW32 # ii
    fsub r0, r3, r0; fmul r3, r1, ra_tw_re+TW32 # ir
                     fmul r1, r2, rb_tw_im+TW32;# ri
    fadd r1, r1, r3;
.endm

##############################################################################
# FFT-16 codelet

# (MM) Optimized: joined load_xxx and ldtmu in FFT-16 codelet
# Extracted stride from load_xxx to make it part of the procedure.
# Saves several instructions and delays ldtmu for linear reads.

# redefine this to compile a fixed stride into the fft_16 bodies.
.set DEF_STRIDE, 0

.macro body_fft_16_rev
    ldtmu0
    mov r0, r4; ldtmu0
    interleave r0, r4
# (MM) Optimized: better use of both ALUs, no need for ra_temp
                               fmul      r2, ra_tw_re+TW16+0, r0
                               fmul      r3, rb_tw_im+TW16+0, r1
    .back 3
    brr -, r:fft_16_cont
    .endb
.endm

.macro body_fft_16_lin
    and.setf -, elem_num, 1<<0
    ldtmu0
    # (MM) Optimized: better use of both ALUs, no need for ra_temp
    mov r0, r4;                fmul      r2, ra_tw_re+TW16+0, r4; ldtmu0
    mov r1, r4;                fmul      r3, rb_tw_im+TW16+0, r4
:fft_16_cont
    .rep i, 3
        fsub.ifnz r0, r2, r3;      fmul.ifnz r2, rb_tw_im+TW16+i, r0
        mov r3, 1<<(i+1);          fmul.ifnz r1, ra_tw_re+TW16+i, r1
        fadd.ifnz r1, r1, r2;      mov.ifz  r2, r0 << (1<<i)
        fadd.ifz  r0, r2, r0;      mov.ifnz r2, r0 >> (1<<i)
        fsub.ifnz r0, r2, r0;      mov.ifz  r2, r1 << (1<<i)
        fadd.ifz  r1, r2, r1;      mov.ifnz r2, r1 >> (1<<i)
        fsub.ifnz r1, r2, r1;      fmul      r2, ra_tw_re+TW16+i+1, r0
        and.setf -, elem_num, r3;  fmul      r3, rb_tw_im+TW16+i+1, r1
    .endr
    # (MM) rotation by 8 left and right is identical => no if required
    # and use -(0-r1) instead of +r1 to make the last instruction unconditional
    # Saves another instruction.
    fsub.ifnz r0, r2, r3;      fmul.ifnz r2, rb_tw_im+TW16+3, r0
    fsub.ifz  r3, 0., r1;      fmul.ifnz r1, ra_tw_re+TW16+3, r1
    fadd.ifnz r1, r1, r2;      mov       r2, r0 << 8
    fadd.ifz  r0, r2, r0;      mov.ifnz  r3, r1
    fsub.ifnz r0, r2, r0;      mov       r2, r1 << 8
    fsub      r1, r2, r3;
    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro bodies_fft_16
:load_fft_16_rev
    read_rev DEF_STRIDE

#:fft_16_rev
    body_fft_16_rev

:load_fft_16_lin
    read_lin DEF_STRIDE

#:fft_16_lin
    body_fft_16_lin
.endm

##############################################################################

.macro bit_rev, shift, mask
    # (MM) Optimized: use MUL ALU for small shifts
    and r1, r0, mask
    .if shift == 1
        shr r0, r0, shift
        and r0, r0, mask;  v8adds r1, r1, r1 # can't overflow because of mask
    .elseif shift == 2
        shr r0, r0, shift; v8adds r1, r1, r1 # can't overflow because of mask
        and r0, r0, mask;  v8adds r1, r1, r1 # can't overflow because of mask
    .else
        shr r0, r0, shift
        and r0, r0, mask
        shl r1, r1, shift
    .endif
    # (MM) Optimized: v8adds can be combined easier; cannot overflow
    v8adds  r0, r0, r1;
.endm

##############################################################################

.macro swizzle
.endm

# and allow instruction combining if re is not r0
.macro interleave, re, im
    .if im != r1
    ;and.setf -, elem_num, 1
    mov.ifnz r1, im;         mov.ifz  r1, re << 1
    mov.ifz  r0, re;         mov.ifnz r0, im >> 1
    .else
    and.setf -, elem_num, 1; mov r2, re
    mov.ifz  r0, r2;         mov.ifnz r0, im >> 1
    mov.ifnz r1, im;         mov.ifz  r1, r2 << 1
    .endif
.endm

##############################################################################

.macro read_rev, stride
    # (MM) Optimized bit swap using regfile A unpack
    # Saves a B register and 3 instruction but no speed.
    ;shl.unpack8a r0, ra_load_idx, 8
    or.unpack8b r0, ra_load_idx, r0

    bit_rev 1, rx_0x5555    # 16 SIMD
.if stride != 0
    # (MM) Optimized: join stride with v8adds
    ;add ra_load_idx, ra_load_idx, stride
.endif
    bit_rev 2, rx_0x3333
    bit_rev 4, rx_0x0F0F
    ;mov r3, 4 # prepare for below

.if STAGES>13               # reversal creates left shift by 16-STAGES
    shl r0, r0, STAGES-13
.endif
.if STAGES<13
    shr r0, r0, 13-STAGES
.endif
    v8adds r2, r0, r3; # r3 = 4

    # (MM) Optimizes: allow alternate source registers for interleave
    interleave r0, r2
    swizzle

    add t0s, ra_addr_x, r0  # r0 = re = {idx[0:STAGES-1], 1'b0, 2'b0}
    add t0s, ra_addr_x, r1  # r1 = im = {idx[0:STAGES-1], 1'b1, 2'b0}
.endm

# (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
.macro load_rev
    read_rev 0
    # (MM) Optimized: joined ldtmu in FFT-16 codelet
    .back 3
    brr ra_link_0, r:fft_16_rev
    .endb
.endm

##############################################################################

.macro read_lin, stride
    shl r0, ra_load_idx, 3; mov r1, ra_load_idx
    # (MM) Optimized: write t0s earlier
    add t0s, r0, ra_addr_x; v8adds r2, ra_addr_x, 4 # can't overflow, LSBs of ra_addr_x are 0
.if stride != 0
    add ra_load_idx, r1, stride
.endif
    add t0s, r0, r2;
.endm

# (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
.macro load_lin
    read_lin 0
    # (MM) Optimized: joined ldtmu in FFT-16 codelet
    .back 3
    brr ra_link_0, r:fft_16_lin
    .endb
.endm

##############################################################################
# Unpack twiddles

.macro unpack_twiddles
    # (MM) Optimized: moved r2/r3 load to next_twiddles_16
# (MM) Optimized: do not rotate r2/r3 unnecessarily at the last turn
.rep i, 3
    ;and.setf -, elem_num, (8>>i)
    mov ra_tw_re+TW16+3-i, r2; mov.ifnz r2, r2 >> (8>>i)
    mov rb_tw_im+TW16+3-i, r3; mov.ifnz r3, r3 >> (8>>i)
.endr
    mov ra_tw_re+TW16+3-3, r2; mov rb_tw_im+TW16+3-3, r3
.endm

##############################################################################
# float-float enhanced-precision subtract (corrects rounding errors)

.macro df64_sub32, a, b     # df64_sub32(float2 &a, float b)
    fsub r0,  a, b              # float2 s = twoSub(a.x, b);
    fsub r1, r0, a
    fadd r2, r1, b
    fsub r1, r0, r1
    fsub r1,  a, r1
    fsub r1, r1, r2

    fadd r1, r1, a+1            # s.y += a.y;

    fadd r2, r0, r1             # a = twoSum(s.x, s,y);
    fsub r2, r2, r0; mov a, r2
    fsub r1, r1, r2
    fsub r2,  a, r2
    fsub r0, r0, r2
    fadd a+1, r0, r1;
.endm

##############################################################################
# Rotate twiddles using enhanced-precision trig recurrence

.macro rotate, base, step
    # (MM) Optimized: better use of both ALUs
    mov r2, ra_tw_re+base;  fmul r1, ra_tw_re+base, rb_tw_im+step # b.cos
    mov r3, rb_tw_im+base;  fmul r0, rb_tw_im+base, ra_tw_re+step # a.sin
    fsub r2, r0, r1;        fmul ra_temp, r2, ra_tw_re+step # a.cos
    df64_sub32 rb_tw_im+base, r2
    ;                       fmul r3, r3, rb_tw_im+step # b.sin
    fadd r2, ra_temp, r3
    df64_sub32 ra_tw_re+base, r2
.endm

.macro next_twiddles_16
    rotate TW16+3, TW16_STEP
    # (MM) Optimized: moved r2/r3 load from unpack_twiddles
    mov r2, ra_tw_re+TW16+3; mov r3, rb_tw_im+TW16+3
    unpack_twiddles
.endm

.macro next_twiddles_32
    rotate TW32, TW32_STEP
    next_twiddles_16
.endm

.macro next_twiddles_64
    rotate TW64, TW64_STEP
    rotate TW48, TW48_STEP
    next_twiddles_32
.endm

##############################################################################
# Alternate input/output buffers between stages

.macro swap_buffers
    mov rb_addr_y, ra_addr_x; mov ra_addr_x, rb_addr_y;
.endm

##############################################################################
# Reset counters and twiddles

# (MM) requires TW16 factors in TMU queue
.macro init_stage, m
    # (MM) Optimized: clear residual registers for >16 in store_tw
    # (MM) Optimized: instruction packing
.if isRegfileB(rx_inst)
    ;mov r0, rx_inst
    shl r0, r0, m
.else
    ;shl r0, rx_inst, m
.endif
    # (MM) Optimized: moved r2/r3 load from unpack_twiddles
    # and avoid intermediate step in TW16+3 registers
    add ra_load_idx, r0, elem_num;  mov rb_tw_im+TW16+4, 0;  ldtmu0
    mov r2, r4;                     mov ra_tw_re+TW16+4, 0;  ldtmu0
    mov r3, r4;
    unpack_twiddles
    mov ra_save_ptr, rb_addr_y;
.endm

# (MM) More powerful and optimized init macros to simplify code

.macro init_base_32, tw16, tw32
    .if tw32
    ;mov r3, rx_tw_shared;
    read_tw r3, tw32
    .else # vc4asm instruction join does not check for register dependencies
    mov t0s, rx_tw_shared;  mov r3, rx_tw_shared
    add t0s, r3, 4
    .endif
    read_tw r3, tw16
    store_tw TW32+0, 0
    init_stage 5
.endm

.macro init_base_64, tw16, tw32, tw48, tw64
    ;mov r3, rx_tw_shared;
    read_tw r3, tw64
    read_tw r3, tw48
    read_tw r3, tw32
    read_tw r3, tw16
    store_tw TW64, 0
    store_tw TW48, 0
    store_tw TW32+0, 0
    init_stage 6
.endm

.macro init_step_16, tw16, st16
    # (MM) Optimized separate preparation of TMU from ldtmu for better pipeline processing.
    mov r3, rx_tw_shared;
    read_tw r3, st16
    read_tw r3, tw16
    swap_buffers
    store_tw TW16_STEP, 0
    init_stage 4
.endm

.macro init_step_32, tw16, tw32, st16, st32
    # (MM) Optimized separate preparation of TMU from ldtmu for better pipeline processing.
    mov r3, rx_tw_shared;
    read_tw r3, st32
    read_tw r3, st16
    read_tw r3, tw32
    read_tw r3, tw16
    swap_buffers
    store_tw TW32_STEP, 0
    store_tw TW16_STEP, 0
    store_tw TW32+0, 1
    init_stage 5
.endm

.macro init_step_64, tw16, tw32, tw48, tw64, st16, st32, st48, st64
    # (MM) Optimized separate preparation of TMU from ldtmu for better pipeline processing.
    mov r3, rx_tw_shared;
    read_tw r3, st64
    read_tw r3, st48
    read_tw r3, tw64
    read_tw r3, tw48
    swap_buffers
    store_tw TW64_STEP, 0
    store_tw TW48_STEP, 0
    store_tw TW64+0, 1
    store_tw TW48+0, 1
    read_tw r3, st32
    read_tw r3, st16
    read_tw r3, tw32
    read_tw r3, tw16
    store_tw TW32_STEP, 0
    store_tw TW16_STEP, 0
    store_tw TW32+0, 1
    init_stage 6
.endm

.macro init_last_16, tw16, st16
    read_tw rx_tw_shared, st16
    read_tw rx_tw_unique, tw16
    swap_buffers
    store_tw TW16_STEP, 0
    init_stage 4
.endm

.macro init_last_32, tw16, tw32, st16, st32
    read_tw rx_tw_shared, st32
    read_tw rx_tw_shared, st16
    ;mov r3, rx_tw_unique;
    read_tw r3, tw32
    read_tw r3, tw16
    swap_buffers
    store_tw TW32_STEP, 0
    store_tw TW16_STEP, 0
    store_tw TW32+0, 1
    init_stage 5
.endm

.macro init_last_64, tw16, tw32, tw48, tw64, st16, st32, st48, st64
    # (MM) Optimized separate preparation of TMU from ldtmu for better pipeline processing.
    mov r3, rx_tw_shared;
    read_tw r3, st64
    read_tw r3, st48
    read_tw r3, st32
    read_tw r3, st16
    swap_buffers
    store_tw TW64_STEP, 0
    store_tw TW48_STEP, 0
    store_tw TW32_STEP, 0
    store_tw TW16_STEP, 0
    ;mov r3, rx_tw_unique;
    read_tw r3, tw64
    read_tw r3, tw48
    read_tw r3, tw32
    read_tw r3, tw16
    store_tw TW64+0, 1
    store_tw TW48+0, 1
    store_tw TW32+0, 1
    init_stage 6
.endm

##############################################################################

.set LOAD_STRAIGHT, 0
.set LOAD_REVERSED, 1

.macro loader_16, stride, mode, back
    # (MM) Optimized: extracted stride from load_xxx to make read part of a procedure
    # (MM) Optimized: clone instructions of branch target instead of nops.
    .if mode==LOAD_REVERSED
        brr_opt ra_link_0, r:load_fft_16_rev, back
    .else
        brr_opt ra_link_0, r:load_fft_16_lin, back
    .endif

    add ra_load_idx, ra_load_idx, stride;
.endm

.macro body_pass_16, mode
    loader_16 rb_0x80, mode, 0

    # (MM) Optimized: use xor for vpm swap (less memory I/O)
    # and save a A register and a B register
    .assert vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0)) == 16
    .assert vdw_setup_0(32, 16, dma_h32(16,0)) - vdw_setup_0(32, 16, dma_h32(0,0)) == 0x800
    ;mov r3, rb_0x80
    mul24 r2, 4, 4;                                  # r2 = 16

    # (MM) Optimized: move write_vpm_16 to body_pass_16
    # and expand inline to pack with VPM swap code, saves 3 instructions
    xor vw_setup,  rx_vpm, r2;     mul24 r3, r2, r3  # r3 = 0x800
    xor ra_vdw_16, ra_vdw_16, r3;  mov vpm, r0
    xor rx_vpm,    rx_vpm, r2;     mov vpm, r1;

    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro body_pass_32, mode
    loader_16 rb_0xF0, mode, 0
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: no need for 2 VPM register sets anymore
    # (MM) Optimized: use xor for vpm swap (less memory I/O)
    # and save a A register and a B register
    .assert vpm_setup(1, 1, v32(32,0)) - vpm_setup(1, 1, v32(0,0)) == 32
    .assert vdw_setup_0(32, 16, dma_h32(32,0)) - vdw_setup_0(32, 16, dma_h32(0,0)) == 0x1000
    ;mov r2, 16
    shl r3, r2, 8;                 v8adds r2, r2, r2  # r3 = 0x1000; r2 = 32
    xor r2, rx_vpm, r2

    # (MM) Optimized: move write_vpm_32 to body_pass_32
    # and expand inline to join with VPM swap code
    xor ra_vdw_32, ra_vdw_32, r3;  mov vw_setup, r2
    fadd vpm, ra_32_re, r0;        mov rx_vpm, r2
    # (MM) Optimized: less memory I/O and prepare for better instruction packing
    fadd vpm, rb_32_im, r1;        mov r3, rb_32_im
    add vw_setup, r2, vpm_setup(1, 1, v32(16,0)) - vpm_setup(1, 1, v32(0,0))
    fsub vpm, ra_32_re, r0
    fsub vpm, r3,       r1;

    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
.endm

.macro body_pass_64, mode, step
    loader_16 0x10, mode, 0
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: less memory I/O
    fadd ra_64+0, ra_32_re, r0; mov r2, ra_32_re
    fsub ra_64+2, r2, r0
    fadd ra_64+1, rb_32_im, r1; mov r2, rb_32_im
    fsub ra_64+3, r2, r1;

    loader_16 step, mode, 3
    mov ra_32_re, r0; mov rb_32_im, r1
    loader_16 0x10, mode, 2
    fft_twiddles_32

    # (MM) Optimized: better use of both ALUs and accumulators
    fadd r3, rb_32_im, r1
    fsub rb_32_im, rb_32_im, r1; fmul r1, r3, ra_tw_re+TW48 # ir
    fadd r2, ra_32_re, r0;       fmul r3, r3, rb_tw_im+TW48 # ii
    fsub ra_32_re, ra_32_re, r0; fmul r0, r2, rb_tw_im+TW48 # ri
    fadd rb_64+1, r0, r1;        fmul r2, r2, ra_tw_re+TW48 # rr
    fsub rb_64+0, r2, r3;        fmul r0, ra_32_re, rb_tw_im+TW64 # ri
    mov r3, rb_32_im;            fmul r1, rb_32_im, ra_tw_re+TW64 # ir
    # (MM) Optimized: easier procedure chains
    # place branch from outside individually
    mov r2, ra_32_re;            fmul r3, r3, rb_tw_im+TW64 # ii
    fadd rb_64+3, r0, r1;        fmul r2, r2, ra_tw_re+TW64 # rr
    fsub rb_64+2, r2, r3;
.endm

##############################################################################

.macro exit, flag
    # (MM) Optimized: no need to wait with thrend
    mov interrupt, flag; thrend
    nop
    nop
.endm

##############################################################################
